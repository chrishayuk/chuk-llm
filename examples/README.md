# chuk-llm Examples

Comprehensive examples demonstrating all features of chuk-llm with modern Pydantic-native architecture.

## ğŸš€ Quick Start - New User Path

**Start here if you're new to chuk-llm!**

### Beginner Examples (5 minutes)
1. **[00_quick_start.py](00_quick_start.py)** - One line to ask a question
2. **[01_basic_ask.py](01_basic_ask.py)** - Basic async and sync patterns
3. **[02_streaming.py](02_streaming.py)** - Real-time streaming responses

### Registry System (10 minutes) - â­ THE KEY DIFFERENTIATOR
4. **[03_registry_discovery.py](03_registry_discovery.py)** - Intelligent model selection
5. **[registry_provider_discovery.py](registry_provider_discovery.py)** - Deep dive into discovery

### Intermediate Examples (15 minutes)
6. **[04_tools_basic.py](04_tools_basic.py)** - Function calling basics
7. **[05_tools_advanced.py](05_tools_advanced.py)** - Auto-execution with Tools class
8. **[06_conversations.py](06_conversations.py)** - Stateful conversations

### Advanced Examples (20 minutes)
9. **[07_json_mode.py](07_json_mode.py)** - Structured outputs and data extraction
10. **[08_multimodal.py](08_multimodal.py)** - Vision and image understanding

## ğŸ“ Directory Structure

The examples directory is now clean and well-organized:

```
examples/
â”œâ”€â”€ README.md                                   # This file
â”‚
â”œâ”€â”€ ğŸ†• CORE EXAMPLES (Root directory - Start Here!)
â”‚   â”œâ”€â”€ 00_quick_start.py                      # Simplest possible example
â”‚   â”œâ”€â”€ 01_basic_ask.py                        # Basic usage patterns
â”‚   â”œâ”€â”€ 02_streaming.py                        # Real-time streaming
â”‚   â”œâ”€â”€ 03_registry_discovery.py               # ğŸ§  Registry-based selection
â”‚   â”œâ”€â”€ 04_tools_basic.py                      # Tool calling basics
â”‚   â”œâ”€â”€ 05_tools_advanced.py                   # Auto-execution
â”‚   â”œâ”€â”€ 06_conversations.py                    # Stateful chatbots
â”‚   â”œâ”€â”€ 07_json_mode.py                        # Structured outputs
â”‚   â”œâ”€â”€ 08_multimodal.py                       # Vision/images
â”‚   â””â”€â”€ registry_provider_discovery.py         # Registry deep dive
â”‚
â”œâ”€â”€ providers/                                  # Provider-specific examples (17 files)
â”‚   â”œâ”€â”€ run_all_providers.py                   # â­ Test all providers
â”‚   â”œâ”€â”€ openai_usage_examples.py               # OpenAI (GPT-5, o1, GPT-4o)
â”‚   â”œâ”€â”€ openai_chat_completions_example.py     # Chat completions format
â”‚   â”œâ”€â”€ openai_responses_example.py            # Responses API (stateful)
â”‚   â”œâ”€â”€ openai_compatible_example.py           # OpenAI-compatible providers
â”‚   â”œâ”€â”€ anthropic_usage_examples.py            # Claude 3.5
â”‚   â”œâ”€â”€ gemini_usage_examples.py               # Gemini 2.0
â”‚   â”œâ”€â”€ groq_usage_examples.py                 # Ultra-fast inference
â”‚   â”œâ”€â”€ mistral_usage_examples.py              # Mistral AI
â”‚   â”œâ”€â”€ azure_usage_examples.py                # Enterprise Azure
â”‚   â”œâ”€â”€ watsonx_usage_examples.py              # IBM Watsonx
â”‚   â”œâ”€â”€ advantage_usage_examples.py            # IBM Advantage
â”‚   â”œâ”€â”€ deepseek_usage_examples.py             # DeepSeek V3
â”‚   â”œâ”€â”€ perplexity_usage_examples.py           # Web search + citations
â”‚   â”œâ”€â”€ openrouter_usage_examples.py           # 100+ models via one API
â”‚   â”œâ”€â”€ conversation_isolation_demo.py         # Architecture deep dive
â”‚   â””â”€â”€ session_isolation_demo.py              # Session management
â”‚
â””â”€â”€ advanced/                                   # Advanced features (6 files)
    â”œâ”€â”€ registry_demo.py                       # Registry system demo
    â”œâ”€â”€ performance_demo.py                    # Performance benchmarks
    â”œâ”€â”€ dynamic_provider_workflow.py           # Dynamic provider selection
    â”œâ”€â”€ streaming_usage.py                     # Advanced streaming
    â”œâ”€â”€ tools_execution_demo.py                # Tool execution patterns
    â””â”€â”€ common_demos.py                        # Common usage patterns
```

**Total: 33 focused, well-organized examples** (10 core + 17 provider + 6 advanced)

### ğŸ§¹ Recent Cleanup (2025-01-21)

The examples directory was massively cleaned up to remove:
- âŒ 34+ duplicate, debug, and outdated examples
- âŒ Pirate-themed examples (fun but not core)
- âŒ Legacy discovery examples (replaced by registry)
- âŒ Test and scratch files

**Result**: Crystal clear learning path with only essential, well-documented examples!

## Test All Providers

Run the master test suite to verify all providers work:

```bash
python providers/run_all_providers.py
python providers/run_all_providers.py --provider openai --verbose
```

## OpenAI Examples Overview ğŸ†•

We provide **3 comprehensive OpenAI examples** covering different use cases:

### 1. OpenAI Chat Completions API

**File**: `openai_chat_completions_example.py`

OpenAI's traditional Chat Completions API (`/v1/chat/completions`):

```bash
python examples/providers/openai_chat_completions_example.py
python examples/providers/openai_chat_completions_example.py --model gpt-5
python examples/providers/openai_chat_completions_example.py --demo 3  # Function calling
```

**Features** (11 demos):
- âœ… Basic completion, streaming, function calling
- âœ… Vision, JSON mode, structured outputs
- âœ… GPT-5 and GPT-5-mini support
- âœ… Model comparison (4 models)
- âœ… Manual conversation history management
- âš ï¸ You manage conversation state yourself

**Use this for**: Standard OpenAI requests, maximum control, custom history management

### 2. OpenAI Responses API ğŸ†•

**File**: `openai_responses_example.py`

OpenAI's next-generation Responses API (`/v1/responses`) with built-in conversation state:

```bash
python examples/providers/openai_responses_example.py
python examples/providers/openai_responses_example.py --model gpt-5
python examples/providers/openai_responses_example.py --demo 3  # Stateful conversation
```

**Features** (13 demos):
- âœ… **Stateful conversations** with `previous_response_id`
- âœ… **Automatic history** management by OpenAI (`store=true`)
- âœ… Response retrieval and deletion by ID
- âœ… Background processing mode
- âœ… JSON mode with full json_schema validation
- âœ… GPT-5 with reasoning configuration
- âœ… Vision, function calling, structured outputs

**Use this for**: Multi-turn conversations, complex workflows, automatic state management

### 3. OpenAI-Compatible Providers

**File**: `openai_compatible_example.py`

Using other providers (Groq, DeepSeek, Together, Mistral, Perplexity) with the same API:

```bash
python examples/providers/openai_compatible_example.py
python examples/providers/openai_compatible_example.py --provider groq
python examples/providers/openai_compatible_example.py --demo 4  # Provider comparison
```

**Features** (6 demos):
- âœ… **5 providers** (Groq, DeepSeek, Together, Mistral, Perplexity)
- âœ… Same API across all providers
- âœ… Basic completion, streaming, function calling
- âœ… Provider comparison and multi-provider apps
- âœ… Easy provider switching (just change base_url + api_key)

**Use this for**: Provider flexibility, cost optimization, avoiding vendor lock-in

---

## API Comparison

| Feature | Chat Completions | Responses API | Compatible Providers |
|---------|------------------|---------------|---------------------|
| **Endpoint** | `/v1/chat/completions` | `/v1/responses` | `/v1/chat/completions` |
| **History** | Manual | âœ… Automatic | Manual |
| **Storage** | None | âœ… Built-in | None |
| **Stateful** | No | âœ… Yes | No |
| **Providers** | OpenAI only | OpenAI only | âœ… Multi-provider |
| **JSON Schema** | JSON mode only | âœ… Full validation | Varies |
| **Background** | No | âœ… Yes | No |
| **Use Case** | Standard requests | Complex conversations | Provider flexibility |

## All 12 Providers

ChukLLM supports 12 LLM providers with modern Pydantic clients (100% coverage):

### Core Providers

| Provider | API Key Env Var | Default Model | Status |
|----------|----------------|---------------|---------|
| **OpenAI** | `OPENAI_API_KEY` | `gpt-4o-mini` | âœ… Modern client |
| **Anthropic** | `ANTHROPIC_API_KEY` | `claude-3-5-haiku-20241022` | âœ… Modern client |

### OpenAI-Compatible Providers

| Provider | API Key Env Var | Default Model | Status |
|----------|----------------|---------------|---------|
| **Groq** | `GROQ_API_KEY` | `llama-3.3-70b-versatile` | âœ… Modern client |
| **DeepSeek** | `DEEPSEEK_API_KEY` | `deepseek-chat` | âœ… Modern client |
| **Together** | `TOGETHER_API_KEY` | `meta-llama/Llama-3.3-70B-Instruct-Turbo` | âœ… Modern client |
| **Perplexity** | `PERPLEXITY_API_KEY` | `llama-3.1-sonar-small-128k-online` | âœ… Modern client |
| **Mistral** | `MISTRAL_API_KEY` | `mistral-small-latest` | âœ… Modern client |
| **Ollama** | `OLLAMA_BASE_URL` (optional) | `llama3.2:latest` | âœ… Modern client |

### Enterprise Providers

| Provider | API Key Env Var | Default Model | Status |
|----------|----------------|---------------|---------|
| **Azure OpenAI** | `AZURE_OPENAI_API_KEY` + `AZURE_OPENAI_ENDPOINT` | `gpt-4o` | âœ… Modern client |
| **IBM Advantage** | `ADVANTAGE_API_KEY` + `ADVANTAGE_API_BASE` | `meta-llama/llama-3-3-70b-instruct` | âœ… Modern client |

### Specialized Providers

| Provider | API Key Env Var | Default Model | Status |
|----------|----------------|---------------|---------|
| **Gemini** | `GEMINI_API_KEY` or `GOOGLE_API_KEY` | `gemini-2.0-flash-exp` | âœ… Modern client |
| **Watsonx** | `WATSONX_API_KEY` + `WATSONX_PROJECT_ID` | `ibm/granite-3-8b-instruct` | âœ… Modern client |

## Features Tested

Each provider example demonstrates:

- âœ… **Basic Completion** - Standard text generation
- âœ… **Streaming** - Real-time response streaming
- âœ… **Tool Calling** - Function/tool invocation (where supported)
- âœ… **System Prompts** - Custom system instructions
- âœ… **Vision** - Multi-modal image understanding (where supported)
- âœ… **JSON Mode** - Structured output (where supported)
- âœ… **Error Handling** - Graceful error management

## Setup

### 1. Install ChukLLM

```bash
pip install -e .
# or with all features
pip install -e .[all]
```

### 2. Set API Keys

Create a `.env` file in the project root:

```bash
# Core providers
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...

# OpenAI-compatible
GROQ_API_KEY=gsk_...
DEEPSEEK_API_KEY=sk-...
TOGETHER_API_KEY=...
PERPLEXITY_API_KEY=pplx-...
MISTRAL_API_KEY=...

# Ollama (optional, defaults to localhost)
OLLAMA_BASE_URL=http://localhost:11434

# Enterprise
AZURE_OPENAI_API_KEY=...
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com

# IBM providers
ADVANTAGE_API_KEY=...
ADVANTAGE_API_BASE=...
WATSONX_API_KEY=...
WATSONX_PROJECT_ID=...

# Google
GEMINI_API_KEY=...
```

### 3. Run Examples

```bash
# Test all available providers
python providers/run_all_providers.py

# Test specific provider
python providers/run_all_providers.py --provider openai --verbose

# Run comprehensive provider examples
python providers/openai_usage_examples.py
python providers/anthropic_usage_examples.py
python providers/gemini_usage_examples.py
```

## Usage Patterns

### Basic Completion

```python
from chuk_llm.api import ask

response = await ask(
    "Explain quantum computing in simple terms",
    provider="openai",
    model="gpt-4o-mini"
)
print(response)
```

### Streaming

```python
from chuk_llm.api import stream

async for chunk in stream(
    "Write a haiku about AI",
    provider="anthropic",
    model="claude-3-5-haiku-20241022"
):
    print(chunk, end="", flush=True)
```

### Tool Calling

```python
from chuk_llm.api import ask

tools = [{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Get weather for a city",
        "parameters": {
            "type": "object",
            "properties": {
                "city": {"type": "string"}
            }
        }
    }
}]

response = await ask(
    "What's the weather in Tokyo?",
    provider="openai",
    tools=tools
)
```

### Vision (Multi-modal)

```python
from chuk_llm.api import ask

response = await ask(
    [
        {"type": "text", "text": "Describe this image"},
        {"type": "image_url", "image_url": {"url": "https://..."}}
    ],
    provider="gemini",
    model="gemini-2.0-flash-exp"
)
```

## Provider-Specific Notes

### OpenAI
- **Latest models**: GPT-5, GPT-5-mini (newest, most capable)
- Full support for GPT-4, GPT-4o, O-series models
- Vision supported with `gpt-5`, `gpt-4o`, `gpt-4-turbo`
- JSON mode available
- Function calling fully supported

### Anthropic (Claude)
- Claude 3.5 Sonnet/Haiku with enhanced reasoning
- Vision supported with Claude 3+ models
- Different tool format than OpenAI (auto-converted)

### Groq
- Ultra-fast inference (<1s responses)
- Llama 3.3, Mixtral, Gemma models
- Tool calling supported

### Gemini
- Google's latest models with multimodal support
- Vision natively integrated
- Uses REST API (no heavy SDK)

### Azure OpenAI
- Enterprise OpenAI deployment
- Requires endpoint + deployment name
- Same models as OpenAI

### Watsonx
- IBM's enterprise LLM platform
- Granite models optimized for business
- Wraps IBM SDK with modern patterns

### Ollama
- Local model hosting
- No API key needed for localhost
- Supports Llama, Mistral, CodeLlama, etc.

## Troubleshooting

### API Key Not Set

```
âŒ OPENAI_API_KEY not set
   export OPENAI_API_KEY='sk-...'
```

**Solution**: Set the required environment variable or add to `.env`

### Model Not Found

```
âŒ Model 'gpt-5' not found
```

**Solution**: Check available models for the provider in their documentation

### Rate Limit Errors

```
âŒ Rate limit exceeded
```

**Solution**: Reduce request frequency or upgrade API tier

### Connection Errors

```
âŒ Connection failed to http://localhost:11434
```

**Solution**: For Ollama, ensure server is running: `ollama serve`

## Testing

Run the comprehensive test suite:

```bash
# Test all providers
python providers/run_all_providers.py

# Test specific provider
python providers/run_all_providers.py --provider gemini --verbose

# Quick test (basic features only)
python providers/run_all_providers.py --quick
```

## Migration Status

âœ… **100% Complete** - All 12 providers migrated to modern Pydantic clients

- Type-safe with Pydantic V2
- Fast JSON with orjson
- Connection pooling with httpx
- Clean architecture (no fallbacks)
- Zero breaking changes

## Contributing

When adding new examples:

1. Place provider-specific examples in `providers/`
2. Name files descriptively: `{provider}_usage_examples.py`
3. Include comprehensive tests for all features
4. Add provider to `run_all_providers.py`
5. Update this README

## See Also

- [Migration Progress](../MIGRATION_PROGRESS.md) - 100% migration details
- [Session Summary](../SESSION_SUMMARY.md) - What was accomplished
- [Documentation](../docs/) - Full API documentation

---

**Last Updated**: 2025-11-19
**Migration Status**: 100% Complete (12/12 providers)
**Modern Clients**: OpenAIClient, AnthropicClient, AzureOpenAIClient, OpenAICompatibleClient, GeminiClient, WatsonxClient
