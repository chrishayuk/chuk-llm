# Optimized Ollama configuration based on discovery results
# Generated on 2025-08-07 14:17:57

ollama:
  api_base: http://localhost:11434
  default_model: granite3.3:latest
  
  models:
  - gemma3:latest
  - gpt-oss:latest
  - granite-embedding:278m
  - granite3.3:latest
  - llama3.1:latest
  - llama3.2:latest
  - llama3:latest
  - mistral-nemo:latest
  - mistral-small:latest
  - mistral:latest
  - mxbai-embed-large:latest
  - qwen3:latest

  model_capabilities:
  - pattern: "^llama3\.1\:latest$"
    features: [reasoning, streaming, system_messages, text, tools]
    max_context_length: 8192
    max_output_tokens: 8192
  - pattern: "^(llama3:latest|granite3.3:latest|granite-embedding:278m)$"
    features: [reasoning, streaming, system_messages, text, tools]
    max_context_length: 8192
    max_output_tokens: 2048
  - pattern: "^llama3\.2\:latest$"
    features: [reasoning, streaming, system_messages, text, tools]
    max_context_length: 128000
    max_output_tokens: 8192
  - pattern: "^qwen3\:latest$"
    features: [reasoning, streaming, system_messages, text, tools]
    max_context_length: 32768
    max_output_tokens: 8192
  - pattern: "^(mistral-small:latest|mistral-nemo:latest|mistral:latest)$"
    features: [streaming, system_messages, text, tools]
    max_context_length: 32768
    max_output_tokens: 8192
  - pattern: "^(gpt-oss:latest|mxbai-embed-large:latest)$"
    features: [streaming, system_messages, text]
    max_context_length: 8192
    max_output_tokens: 2048
  - pattern: "^gemma3\:latest$"
    features: [streaming, system_messages, text, tools]
    max_context_length: 8192
    max_output_tokens: 2048