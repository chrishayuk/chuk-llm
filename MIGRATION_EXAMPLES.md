# Migration Examples
## Before & After Code Comparisons

This document shows concrete examples of what needs to change during migration.

---

## 1. Provider Client Interface

### ‚ùå Before (Legacy - openai_client.py)

```python
from chuk_llm.llm.core.base import BaseLLMClient

class OpenAILLMClient(BaseLLMClient):
    def create_completion(
        self,
        messages: list[dict[str, Any]],  # ‚ùå Dict goop
        tools: list[dict[str, Any]] | None = None,  # ‚ùå Dict goop
        *,
        stream: bool = False,
        temperature: float | None = None,  # ‚ùå Primitive parameters
        max_tokens: int | None = None,
        **kwargs: Any,
    ) -> AsyncIterator[dict[str, Any]] | Any:  # ‚ùå Confusing return type
        """Not actually an async method despite returning async types"""

        # ‚ùå Manual dict building with magic strings
        request_data = {
            "model": self.model,
            "messages": messages,  # No validation
            "temperature": temperature,
            "max_tokens": max_tokens,
        }

        if tools:
            request_data["tools"] = tools  # ‚ùå Magic string "tools"

        # ‚ùå Returns dict with magic strings
        if stream:
            return self._stream(request_data)
        else:
            return self._complete(request_data)
```

**Problems**:
- 25+ `dict[str, Any]` in file
- 50+ magic strings (`"role"`, `"content"`, `"tool_calls"`, etc.)
- No validation on input
- Confusing async pattern (sync method returning async types)
- Manually builds request dicts
- Returns untyped dicts

---

### ‚úÖ After (Modern - clients/openai.py)

```python
from chuk_llm.clients.base import AsyncLLMClient
from chuk_llm.core import (
    CompletionRequest,
    CompletionResponse,
    OpenAIEndpoint,
    RequestParam,
    ResponseKey,
)

class OpenAIClient(AsyncLLMClient):
    async def complete(  # ‚úÖ Properly async
        self, request: CompletionRequest  # ‚úÖ Pydantic input, validated at creation
    ) -> CompletionResponse:  # ‚úÖ Pydantic output
        """Type-safe completion with validation"""

        # ‚úÖ Uses APIRequest Pydantic model with zero magic strings
        api_request = self._prepare_request(request)

        # ‚úÖ Actually async call
        response = await self._post_json(
            OpenAIEndpoint.CHAT_COMPLETIONS.value,
            api_request.model_dump(exclude_none=True)
        )

        # ‚úÖ Returns validated Pydantic model
        return self._parse_completion_response(response)

    def _prepare_request(self, request: CompletionRequest) -> APIRequest:
        """Convert to API format with zero magic strings"""
        params: dict[str, Any] = {
            RequestParam.MODEL.value: request.model or self.model,  # ‚úÖ Enum
            RequestParam.MESSAGES.value: [
                self._message_to_dict(msg) for msg in request.messages
            ],
        }

        if request.temperature is not None:
            params[RequestParam.TEMPERATURE.value] = request.temperature  # ‚úÖ Enum

        if request.tools:
            params[RequestParam.TOOLS.value] = [  # ‚úÖ Enum
                loads(tool.model_dump_json()) for tool in request.tools
            ]

        return APIRequest(**params)  # ‚úÖ Returns Pydantic model
```

**Benefits**:
- 0 `dict[str, Any]` in signatures
- 0 magic strings (all enums)
- Full validation via Pydantic
- Proper async/await pattern
- Type-safe throughout
- IDE autocomplete works

---

## 2. Streaming Implementation

### ‚ùå Before (Legacy)

```python
def create_completion(self, messages, stream=False, **kwargs):
    """Sync method returning async iterator - confusing!"""
    if stream:
        return self._stream_completion(messages, **kwargs)

async def _stream_completion(self, messages, **kwargs):
    """Actually async but called from sync method"""
    async for chunk in self.client.chat.completions.create(
        messages=messages,  # ‚ùå No validation
        stream=True,
        **kwargs
    ):
        # ‚ùå Manual dict building with magic strings
        delta = chunk.choices[0].delta

        result = {}
        if delta.content:
            result["content"] = delta.content  # ‚ùå Magic string

        if delta.tool_calls:
            result["tool_calls"] = [  # ‚ùå Magic string
                {
                    "id": tc.id,  # ‚ùå Magic string
                    "type": "function",  # ‚ùå Magic string
                    "function": {  # ‚ùå Magic string
                        "name": tc.function.name,  # ‚ùå Magic string
                        "arguments": tc.function.arguments  # ‚ùå Magic string
                    }
                }
                for tc in delta.tool_calls
            ]

        yield result  # ‚ùå Returns unvalidated dict
```

**Problems**:
- Mixed async/sync pattern
- 15+ magic strings per chunk
- No type safety
- Manual dict construction
- No validation

---

### ‚úÖ After (Modern)

```python
async def stream(  # ‚úÖ Properly async
    self, request: CompletionRequest  # ‚úÖ Validated input
) -> AsyncIterator[StreamChunk]:  # ‚úÖ Type-safe output
    """Zero-copy streaming with validation"""

    api_request = self._prepare_request(request)
    tool_calls_acc: dict[int, dict[str, Any]] = {}

    async for chunk_bytes in self._stream_post(  # ‚úÖ Actually async
        OpenAIEndpoint.CHAT_COMPLETIONS.value,  # ‚úÖ Enum, not magic string
        api_request.model_dump(exclude_none=True)
    ):
        chunk_str = chunk_bytes.decode("utf-8").strip()

        if chunk_str.startswith(SSEPrefix.DATA.value):  # ‚úÖ Enum
            chunk_str = chunk_str[len(SSEPrefix.DATA.value):]

        chunk_data = loads(chunk_str)  # ‚úÖ Fast JSON

        # ‚úÖ Parse to Pydantic model with zero magic strings
        stream_chunk = self._parse_stream_chunk(chunk_data, tool_calls_acc)

        if stream_chunk:
            yield stream_chunk  # ‚úÖ Returns validated StreamChunk

def _parse_stream_chunk(
    self, chunk: dict[str, Any], tool_calls_acc: dict[int, dict[str, Any]]
) -> StreamChunk | None:
    """Parse with zero magic strings"""
    choice = chunk[ResponseKey.CHOICES.value][0]  # ‚úÖ Enum
    delta = choice.get(ResponseKey.DELTA.value, {})  # ‚úÖ Enum

    content = delta.get(ResponseKey.CONTENT.value)  # ‚úÖ Enum

    # ‚úÖ Build validated Pydantic model
    if content or complete_tool_calls or finish_reason:
        return StreamChunk(  # ‚úÖ Pydantic model
            content=content,
            tool_calls=complete_tool_calls,
            finish_reason=finish_reason,
        )

    return None
```

**Benefits**:
- Proper async generator
- 0 magic strings (all enums)
- Full type safety
- Validation at parse time
- Zero-copy (yields immediately)

---

## 3. Message Construction

### ‚ùå Before (Legacy)

```python
# ‚ùå Building messages manually with magic strings
messages = [
    {
        "role": "system",  # ‚ùå Magic string
        "content": "You are helpful"  # ‚ùå Magic string
    },
    {
        "role": "user",  # ‚ùå Magic string
        "content": prompt  # ‚ùå Magic string
    }
]

# ‚ùå No validation - typos cause runtime errors
messages.append({
    "rol": "assistant",  # ‚ùå TYPO! Will fail at runtime
    "content": "Hello"
})
```

---

### ‚úÖ After (Modern)

```python
from chuk_llm.core import Message, MessageRole

# ‚úÖ Type-safe, validated at creation
messages = [
    Message(
        role=MessageRole.SYSTEM,  # ‚úÖ Enum - IDE autocomplete
        content="You are helpful"
    ),
    Message(
        role=MessageRole.USER,  # ‚úÖ Enum
        content=prompt
    )
]

# ‚úÖ Typos caught at validation time
try:
    messages.append(Message(
        rol=MessageRole.ASSISTANT,  # ‚ùå Caught by Pydantic!
        content="Hello"
    ))
except ValidationError as e:
    print(f"Invalid message: {e}")  # Fails fast with clear error
```

**Benefits**:
- Typos caught immediately
- IDE autocomplete on fields
- Immutable (frozen=True)
- Self-documenting

---

## 4. Tool Call Handling

### ‚ùå Before (Legacy)

```python
# ‚ùå Checking tool calls with magic strings
if "tool_calls" in response:  # ‚ùå Magic string, runtime check
    for tc in response["tool_calls"]:  # ‚ùå No type info
        func_name = tc["function"]["name"]  # ‚ùå Nested magic strings
        func_args = tc["function"]["arguments"]  # ‚ùå More magic

        # ‚ùå String comparison, typos possible
        if func_name == "get_weather":
            args = json.loads(func_args)  # ‚ùå Manual parsing
            location = args["location"]  # ‚ùå Another magic string
```

---

### ‚úÖ After (Modern)

```python
from chuk_llm.core import CompletionResponse, ToolType

response: CompletionResponse = await client.complete(request)

# ‚úÖ Type-safe access with autocomplete
if response.tool_calls:  # ‚úÖ Property access, not dict lookup
    for tc in response.tool_calls:  # ‚úÖ tc is ToolCall model
        func_name = tc.function.name  # ‚úÖ Type-safe property
        func_args = tc.function.arguments  # ‚úÖ Already validated JSON string

        # ‚úÖ Enum comparison
        if tc.type == ToolType.FUNCTION:
            args = loads(func_args)  # ‚úÖ Fast JSON
            # args is now typed dict from JSON schema
```

**Benefits**:
- No dict lookups
- Full IDE support
- Type checker validates all access
- Immutable models

---

## 5. API Integration

### ‚ùå Before (Legacy - api/core.py)

```python
async def ask(
    prompt: str,
    provider: str = "openai",
    model: str | None = None,
    **kwargs
) -> dict[str, Any]:  # ‚ùå Returns untyped dict
    """Ask with dict everywhere"""

    # ‚ùå Build message dict manually
    messages = [
        {"role": "user", "content": prompt}  # ‚ùå Magic strings
    ]

    # ‚ùå Get legacy client
    client = get_client(provider, model)

    # ‚ùå Pass dicts, get dict back
    result = await client.create_completion(
        messages=messages,
        **kwargs
    )

    # ‚ùå Extract with magic strings
    return {
        "response": result.get("content", ""),  # ‚ùå Magic string
        "tool_calls": result.get("tool_calls", [])  # ‚ùå Magic string
    }
```

---

### ‚úÖ After (Modern)

```python
from chuk_llm.core import CompletionRequest, CompletionResponse, Message, MessageRole
from chuk_llm.compat import dict_to_completion_request, completion_response_to_dict

async def ask(
    prompt: str,
    provider: str = "openai",
    model: str | None = None,
    **kwargs
) -> dict[str, Any]:  # Keep dict for backward compat at API boundary
    """Type-safe internally, dict at boundary"""

    # ‚úÖ Build validated Pydantic request
    request = CompletionRequest(
        messages=[
            Message(role=MessageRole.USER, content=prompt)  # ‚úÖ Type-safe
        ],
        model=model,
        **kwargs  # Pydantic validates these
    )

    # ‚úÖ Get modern client
    client = get_modern_client(provider)  # Returns AsyncLLMClient

    # ‚úÖ Type-safe call
    response: CompletionResponse = await client.complete(request)

    # ‚úÖ Convert to dict only at boundary (backward compat)
    return completion_response_to_dict(response)
```

**Benefits**:
- Type-safe internally
- Validation at request creation
- Backward compatible API
- Clear migration path

---

## 6. Error Handling

### ‚ùå Before (Legacy)

```python
# ‚ùå Checking dict keys for errors
if "error" in response:  # ‚ùå Magic string
    error_type = response.get("error_type", "unknown")  # ‚ùå Magic string
    error_msg = response.get("error_message", "")  # ‚ùå Magic string
    raise Exception(f"{error_type}: {error_msg}")  # ‚ùå Untyped exception
```

---

### ‚úÖ After (Modern)

```python
from chuk_llm.core import LLMError, ErrorType

try:
    response = await client.complete(request)
except LLMError as e:  # ‚úÖ Typed exception
    # ‚úÖ Type-safe properties
    if e.error_type == ErrorType.RATE_LIMIT_ERROR.value:  # ‚úÖ Enum
        if e.retry_after:  # ‚úÖ Type-safe property
            await asyncio.sleep(e.retry_after)

    logger.error(f"{e.error_type}: {e.error_message}")
    raise  # Re-raise with full context
```

**Benefits**:
- Structured exceptions
- Type-safe error info
- Retry logic built-in
- Clear error handling

---

## Summary of Changes

| Aspect | Before (Legacy) | After (Modern) | Improvement |
|--------|----------------|----------------|-------------|
| **Type Safety** | `dict[str, Any]` | Pydantic models | 100% type coverage |
| **Magic Strings** | 50+ per file | 0 (all enums) | Zero runtime typos |
| **Async Pattern** | Sync returning async | Proper `async def` | Clear, idiomatic |
| **Validation** | Runtime (if at all) | Parse time | Fail fast |
| **IDE Support** | Minimal | Full autocomplete | 10x productivity |
| **Performance** | stdlib json | orjson (2-3x faster) | 3x speedup |
| **Testability** | Hard (dict mocks) | Easy (Pydantic factories) | 5x easier |
| **Maintainability** | Fragile | Robust | Refactor-safe |

---

## Migration Checklist

For each provider file:

- [ ] Replace `BaseLLMClient` with `AsyncLLMClient`
- [ ] Change `create_completion()` to `async def complete(request: CompletionRequest)`
- [ ] Change streaming to `async def stream(request: CompletionRequest)`
- [ ] Replace all `dict[str, Any]` with Pydantic models
- [ ] Replace magic strings with enums from `core.constants` and `core.enums`
- [ ] Use `APIRequest` model for API requests
- [ ] Return `CompletionResponse` from `complete()`
- [ ] Yield `StreamChunk` from `stream()`
- [ ] Add type hints everywhere
- [ ] Update tests to use Pydantic models
- [ ] Run `make check` to verify

**Result**: Type-safe, fast, maintainable code with zero magic strings! üöÄ
