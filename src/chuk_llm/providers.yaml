##############################################################################
# chuk-llm  –  External provider configuration
#
# • Any key here overrides or extends the built-in defaults in chuk_llm.
# • The special field `inherits:` copies every value from the named provider
#   before applying the overrides that follow it.               ──────────────
# • Environment variables listed in *_env fields are read at runtime; you
#   don't store secrets inside this file.
# • NEW: `models` lists available models for dynamic function generation
# • NEW: `model_aliases` creates convenient short function names                                       
##############################################################################

# ─────────────────────── Global defaults ────────────────────────
__global__:
  # Which provider + model should be used when the caller does not specify?
  active_provider: openai
  active_model: gpt-4o-mini


# ───────────────────────── Providers ────────────────────────────
# OpenAI – most popular models for function generation
openai:
  client: chuk_llm.llm.providers.openai_client.OpenAILLMClient
  api_key_env: OPENAI_API_KEY
  default_model: gpt-4o-mini
  models:
    - gpt-4.1
    - gpt-4.1-mini
    - gpt-4.1-nano
    - gpt-4o
    - gpt-4o-mini
    - gpt-4-turbo
    - gpt-4
    - gpt-3.5-turbo
    - o1
    - o1-mini
    - o3
    - o3-mini
    - o4-mini
  model_aliases:
    # Ultra-short aliases
    gpt4o: gpt-4o
    gpt4o_mini: gpt-4o-mini
    gpt4_turbo: gpt-4-turbo
    gpt4: gpt-4
    gpt3_5: gpt-3.5-turbo
    gpt4_1: gpt-4.1
    # O-series aliases
    o1_mini: o1-mini
    o3_mini: o3-mini
    o4_mini: o4-mini
    # Version aliases
    latest: gpt-4.1
    mini: gpt-4.1-mini
    nano: gpt-4.1-nano
    turbo: gpt-4-turbo


# DeepSeek – OpenAI-compatible endpoint (inherits everything, just override)
deepseek:
  inherits: openai
  client: chuk_llm.llm.providers.openai_client.OpenAILLMClient
  api_key_env: DEEPSEEK_API_KEY
  api_base: https://api.deepseek.com
  default_model: deepseek-chat
  models:
    - deepseek-chat
    - deepseek-reasoner
  model_aliases:
    chat: deepseek-chat
    reasoner: deepseek-reasoner
    # Descriptive aliases
    thinking: deepseek-reasoner
    conversation: deepseek-chat
    default: deepseek-chat


# Anthropic – Claude models
anthropic:
  client: chuk_llm.llm.providers.anthropic_client.AnthropicLLMClient
  api_key_env: ANTHROPIC_API_KEY
  default_model: claude-sonnet-4-20250514
  models:
    - claude-sonnet-4-20250514
    - claude-3-5-sonnet-20241022
    - claude-3-sonnet-20240229
    - claude-3-opus-20240229
    - claude-3-haiku-20240307
  model_aliases:
    # Version aliases
    sonnet4: claude-sonnet-4-20250514
    sonnet35: claude-3-5-sonnet-20241022
    sonnet3: claude-3-sonnet-20240229
    opus: claude-3-opus-20240229
    haiku: claude-3-haiku-20240307
    # Simple aliases
    sonnet: claude-sonnet-4-20250514
    latest: claude-sonnet-4-20250514


# Groq – OpenAI style, ultra-fast inference
groq:
  inherits: openai
  client: chuk_llm.llm.providers.groq_client.GroqAILLMClient 
  api_key_env: GROQ_API_KEY
  api_base: https://api.groq.com
  default_model: llama-3.3-70b-versatile
  models:
    - llama-3.3-70b-versatile
    - llama-3.1-8b-instant
    - llama-3.1-70b-versatile
    - mixtral-8x7b-32768
    - gemma2-9b-it
  model_aliases:
    # Version aliases
    llama33_70b: llama-3.3-70b-versatile
    llama31_8b: llama-3.1-8b-instant
    llama31_70b: llama-3.1-70b-versatile
    mixtral: mixtral-8x7b-32768
    gemma2: gemma2-9b-it
    # Simple aliases
    llama: llama-3.3-70b-versatile
    llama_big: llama-3.3-70b-versatile
    llama_small: llama-3.1-8b-instant
    # Speed aliases
    instant: llama-3.1-8b-instant
    fast: llama-3.1-8b-instant
    powerful: llama-3.3-70b-versatile
    latest: llama-3.3-70b-versatile


# Google Gemini
gemini:
  client: chuk_llm.llm.providers.gemini_client.GeminiLLMClient
  api_key_env: GOOGLE_API_KEY
  default_model: gemini-2.0-flash
  models:
    - gemini-2.0-flash
    - gemini-1.5-pro
    - gemini-1.5-flash
    - gemini-1.5-flash-8b
  model_aliases:
    # Version aliases
    flash2: gemini-2.0-flash
    pro15: gemini-1.5-pro
    flash15: gemini-1.5-flash
    flash8b: gemini-1.5-flash-8b
    # Simple aliases
    flash: gemini-2.0-flash
    pro: gemini-1.5-pro
    latest: gemini-2.0-flash
    # Capability aliases
    fast: gemini-2.0-flash
    smart: gemini-1.5-pro
    efficient: gemini-1.5-flash-8b


# Mistral AI – official cloud
mistral:
  client: chuk_llm.llm.providers.mistral_client.MistralLLMClient
  api_key_env: MISTRAL_API_KEY
  default_model: mistral-large-latest
  models:
    - mistral-large-latest
    - mistral-medium-latest
    - mistral-small-latest
    - pixtral-large-latest
    - codestral-latest
  model_aliases:
    # Size aliases
    large: mistral-large-latest
    medium: mistral-medium-latest
    small: mistral-small-latest
    # Specialized aliases
    pixtral: pixtral-large-latest
    codestral: codestral-latest
    vision: pixtral-large-latest
    code: codestral-latest
    # Capability aliases
    powerful: mistral-large-latest
    balanced: mistral-medium-latest
    fast: mistral-small-latest
    latest: mistral-large-latest


# Local Ollama daemon (no key needed)
ollama:
  client: chuk_llm.llm.providers.ollama_client.OllamaLLMClient
  api_base: http://localhost:11434
  default_model: qwen3
  models:
    - llama3.3
    - qwen3
    - granite3.3
    - mistral
    - codellama
    - phi3
  model_aliases:
    # Simple aliases
    llama: llama3.3
    qwen: qwen3
    granite: granite3.3
    mistral_local: mistral
    code: codellama
    phi: phi3
    # Capability aliases
    smart: qwen3
    fast: phi3
    creative: llama3.3
    programming: codellama
    latest: qwen3
    default: qwen3


# IBM watsonx.ai
watsonx:
  client: chuk_llm.llm.providers.watsonx_client.WatsonxLLMClient
  api_key_env: WATSONX_API_KEY
  api_key_fallback_env: IBM_CLOUD_API_KEY
  watsonx_ai_url: https://us-south.ml.cloud.ibm.com
  project_id_env: WATSONX_PROJECT_ID
  space_id_env:   WATSONX_SPACE_ID
  default_model: ibm/granite-3-8b-instruct
  models:
    - ibm/granite-3-8b-instruct
    - ibm/granite-3-2b-instruct
    - meta-llama/llama-3-2-1b-instruct
    - meta-llama/llama-3-2-3b-instruct
    - mistralai/mistral-large
  model_aliases:
    # Granite aliases
    granite8b: ibm/granite-3-8b-instruct
    granite2b: ibm/granite-3-2b-instruct
    granite: ibm/granite-3-8b-instruct
    # Llama aliases
    llama1b: meta-llama/llama-3-2-1b-instruct
    llama3b: meta-llama/llama-3-2-3b-instruct
    llama: meta-llama/llama-3-2-3b-instruct
    # Mistral aliases
    mistral_wx: mistralai/mistral-large
    mistral: mistralai/mistral-large
    # Capability aliases
    small: ibm/granite-3-2b-instruct
    medium: ibm/granite-3-8b-instruct
    large: mistralai/mistral-large
    efficient: meta-llama/llama-3-2-1b-instruct
    latest: ibm/granite-3-8b-instruct


# Perplexity – OpenAI-style endpoint
perplexity:
  inherits: openai
  client: chuk_llm.llm.providers.openai_client.OpenAILLMClient
  api_key_env: PERPLEXITY_API_KEY
  api_base: https://api.perplexity.ai
  default_model: sonar-pro
  models:
    - sonar-pro
    - sonar-reasoning
    - sonar-huge
    - llama-3.1-sonar-large-128k-online
    - llama-3.1-sonar-small-128k-online
  model_aliases:
    # Simple aliases
    pro: sonar-pro
    reasoning: sonar-reasoning
    huge: sonar-huge
    large: llama-3.1-sonar-large-128k-online
    small: llama-3.1-sonar-small-128k-online
    # Descriptive aliases
    smart: sonar-reasoning
    powerful: sonar-huge
    online: llama-3.1-sonar-large-128k-online
    search: sonar-pro
    latest: sonar-pro


# (Example) Together AI – another OpenAI-compatible host
togetherai:
  inherits: openai
  client: chuk_llm.llm.providers.openai_client.OpenAILLMClient
  api_key_env: TOGETHERAI_API_KEY
  api_base: https://api.together.xyz
  default_model: meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
  models:
    - meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
    - meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
    - mistralai/Mixtral-8x7B-Instruct-v0.1
    - Qwen/Qwen2.5-72B-Instruct-Turbo
  model_aliases:
    # Model aliases
    llama70b: meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
    llama8b: meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
    mixtral8x7b: mistralai/Mixtral-8x7B-Instruct-v0.1
    qwen72b: Qwen/Qwen2.5-72B-Instruct-Turbo
    # Simple aliases
    llama: meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
    mixtral: mistralai/Mixtral-8x7B-Instruct-v0.1
    qwen: Qwen/Qwen2.5-72B-Instruct-Turbo
    # Capability aliases
    powerful: meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo
    fast: meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
    smart: Qwen/Qwen2.5-72B-Instruct-Turbo
    latest: meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo


##############################################################################
# Global Model Aliases (cross-provider shortcuts)
# 
# These create ultra-convenient functions that work across providers:
# ask_gpt4(), ask_claude(), ask_llama(), etc.
##############################################################################
__global_aliases__:
  # Ultra-short aliases for most popular models
  gpt4: openai/gpt-4o
  gpt4_mini: openai/gpt-4o-mini
  gpt3: openai/gpt-3.5-turbo
  
  # Anthropic aliases
  claude: anthropic/claude-sonnet-4-20250514
  claude4: anthropic/claude-sonnet-4-20250514
  claude_opus: anthropic/claude-3-opus-20240229
  claude_haiku: anthropic/claude-3-haiku-20240307
  
  # Open source models
  llama: groq/llama-3.3-70b-versatile
  llama_fast: groq/llama-3.1-8b-instant
  mixtral: groq/mixtral-8x7b-32768
  
  # Google models
  gemini: gemini/gemini-2.0-flash
  gemini_pro: gemini/gemini-1.5-pro
  
  # Specialized models
  mistral: mistral/mistral-large-latest
  deepseek: deepseek/deepseek-chat
  deepseek_reasoning: deepseek/deepseek-reasoner
  
  # Local models
  qwen: ollama/qwen3
  granite: ollama/granite3.3
  phi: ollama/phi3
  codellama: ollama/codellama
  
  # Enterprise models
  watsonx: watsonx/ibm/granite-3-8b-instruct
  perplexity: perplexity/sonar-pro
  
  # Capability-based aliases
  fastest: groq/llama-3.1-8b-instant
  smartest: anthropic/claude-3-opus-20240229
  cheapest: openai/gpt-4o-mini
  creative: anthropic/claude-3-opus-20240229
  coding: ollama/codellama
  reasoning: deepseek/deepseek-reasoner
  search: perplexity/sonar-pro

##############################################################################
# This configuration now enables super convenient function calls like:
#
# # Ultra-short global aliases
# ask_gpt4_sync("Hello")              # -> OpenAI GPT-4o
# ask_claude_sync("Hello")            # -> Anthropic Claude Sonnet 4
# ask_llama_sync("Hello")             # -> Groq Llama 3.3 70B
# ask_smartest_sync("Complex task")   # -> Claude Opus (best reasoning)
# ask_fastest_sync("Quick question")  # -> Groq Llama 8B (ultra fast)
# ask_coding_sync("Fix this code")    # -> CodeLlama (specialized)
#
# # Provider-specific with aliases
# ask_openai_fast_sync("Hello")       # -> GPT-4o Mini
# ask_anthropic_creative_sync("...")  # -> Claude Opus
# ask_groq_instant_sync("Quick")      # -> Llama 8B instant
# ask_ollama_smart_sync("Think")      # -> Qwen3
#
# # Full model names still work
# ask_openai_gpt_4o_sync("Hello")     # -> OpenAI GPT-4o
##############################################################################